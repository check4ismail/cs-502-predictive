{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1316aeda",
   "metadata": {},
   "source": [
    "# Assignment 11: Split Data\n",
    "Split Data\\\n",
    "Ismail Abdo Elmaliki\\\n",
    "CS 502 - Predictive Analytics\\\n",
    "Capitol Technology University\\\n",
    "Professor Frank Neugebauer\\\n",
    "March 17, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07277d28",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "*Data Understanding*\n",
    "- Info and Head\n",
    "- Skew\n",
    "\n",
    "*Feature Engineering*\n",
    "- Rename columns\n",
    "- Encoding Category Features\n",
    "- Resolving Positive Skewness\n",
    "\n",
    "*Prediction Model*\n",
    "- Random Forest Classifier - Setting up function\n",
    "- Results Evaluation\n",
    "- **Confusion Matrix**\n",
    "- **Undersampling Data**\n",
    "- **Confusion Matrix (Post Undersampling)**\n",
    "- **Hyperparameter Tuning**\n",
    "- **Prediction and Confusion Matrix Wrap Up**\n",
    "\n",
    "*Conclusion*\n",
    "\n",
    "*References*\n",
    "\n",
    "**NOTE**: Anything bolded within Table of Contents indicates new content explicitly for assignment 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d11155",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b7d73a",
   "metadata": {},
   "source": [
    "### Info and Head\n",
    "Taking a look at the data at a high level here are some observations:\n",
    "- `Agency` is of object type -> will need to apply feature engineering and change to numerical values\n",
    "- `Agency Type` is of object type -> will need to apply feature engineering and change to numerical values\n",
    "- `Distribution Channel` is of object type -> will need to apply feature engineering and change to numerical values\n",
    "- `Product Name` is of object type -> will need to apply feature engineering and change to numerical values\n",
    "- `Claim` is of object type -> will need to apply feature engineering and change to numerical values\n",
    "- `Destination` is of object type -> will need to apply feature engineering and change to numerical values\n",
    "- `Gender` is of object type -> will need to apply feature engineering and change to numerical values; also there are missing values which will need to be filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 986,
   "id": "48c165fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 63326 entries, 0 to 63325\n",
      "Data columns (total 11 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   Agency                63326 non-null  object \n",
      " 1   Agency Type           63326 non-null  object \n",
      " 2   Distribution Channel  63326 non-null  object \n",
      " 3   Product Name          63326 non-null  object \n",
      " 4   Claim                 63326 non-null  object \n",
      " 5   Duration              63326 non-null  int64  \n",
      " 6   Destination           63326 non-null  object \n",
      " 7   Net Sales             63326 non-null  float64\n",
      " 8   Commision (in value)  63326 non-null  float64\n",
      " 9   Gender                18219 non-null  object \n",
      " 10  Age                   63326 non-null  int64  \n",
      "dtypes: float64(2), int64(2), object(7)\n",
      "memory usage: 5.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Agency</th>\n",
       "      <th>Agency Type</th>\n",
       "      <th>Distribution Channel</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Claim</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Net Sales</th>\n",
       "      <th>Commision (in value)</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CBH</td>\n",
       "      <td>Travel Agency</td>\n",
       "      <td>Offline</td>\n",
       "      <td>Comprehensive Plan</td>\n",
       "      <td>No</td>\n",
       "      <td>186</td>\n",
       "      <td>MALAYSIA</td>\n",
       "      <td>-29.0</td>\n",
       "      <td>9.57</td>\n",
       "      <td>F</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CBH</td>\n",
       "      <td>Travel Agency</td>\n",
       "      <td>Offline</td>\n",
       "      <td>Comprehensive Plan</td>\n",
       "      <td>No</td>\n",
       "      <td>186</td>\n",
       "      <td>MALAYSIA</td>\n",
       "      <td>-29.0</td>\n",
       "      <td>9.57</td>\n",
       "      <td>F</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CWT</td>\n",
       "      <td>Travel Agency</td>\n",
       "      <td>Online</td>\n",
       "      <td>Rental Vehicle Excess Insurance</td>\n",
       "      <td>No</td>\n",
       "      <td>65</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>-49.5</td>\n",
       "      <td>29.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CWT</td>\n",
       "      <td>Travel Agency</td>\n",
       "      <td>Online</td>\n",
       "      <td>Rental Vehicle Excess Insurance</td>\n",
       "      <td>No</td>\n",
       "      <td>60</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>-39.6</td>\n",
       "      <td>23.76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CWT</td>\n",
       "      <td>Travel Agency</td>\n",
       "      <td>Online</td>\n",
       "      <td>Rental Vehicle Excess Insurance</td>\n",
       "      <td>No</td>\n",
       "      <td>79</td>\n",
       "      <td>ITALY</td>\n",
       "      <td>-19.8</td>\n",
       "      <td>11.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Agency    Agency Type Distribution Channel                     Product Name  \\\n",
       "0    CBH  Travel Agency              Offline               Comprehensive Plan   \n",
       "1    CBH  Travel Agency              Offline               Comprehensive Plan   \n",
       "2    CWT  Travel Agency               Online  Rental Vehicle Excess Insurance   \n",
       "3    CWT  Travel Agency               Online  Rental Vehicle Excess Insurance   \n",
       "4    CWT  Travel Agency               Online  Rental Vehicle Excess Insurance   \n",
       "\n",
       "  Claim  Duration Destination  Net Sales  Commision (in value) Gender  Age  \n",
       "0    No       186    MALAYSIA      -29.0                  9.57      F   81  \n",
       "1    No       186    MALAYSIA      -29.0                  9.57      F   71  \n",
       "2    No        65   AUSTRALIA      -49.5                 29.70    NaN   32  \n",
       "3    No        60   AUSTRALIA      -39.6                 23.76    NaN   32  \n",
       "4    No        79       ITALY      -19.8                 11.88    NaN   41  "
      ]
     },
     "execution_count": 986,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('travel_insurance.csv')\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c85e0",
   "metadata": {},
   "source": [
    "### Skew\n",
    "We can also see below that all numerical type columns are positively skewed. Specifically the columns `Duration`, `Net Sales`, `Commision (in value)`, and `Age`. This is something to keep in mind as feature engineering is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "id": "9060ec6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h8/gz6p2r657dbgvclv0p0zxy180000gn/T/ipykernel_46050/1665899112.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df.skew()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Duration                23.179617\n",
       "Net Sales                3.272373\n",
       "Commision (in value)     4.032269\n",
       "Age                      2.987710\n",
       "dtype: float64"
      ]
     },
     "execution_count": 987,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b87028",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcf99cc",
   "metadata": {},
   "source": [
    "### Rename columns\n",
    "Let's start by renaming columns, making sure they're all lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "id": "a5f6752d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency</th>\n",
       "      <th>agency_type</th>\n",
       "      <th>distribution</th>\n",
       "      <th>product_name</th>\n",
       "      <th>claim</th>\n",
       "      <th>duration</th>\n",
       "      <th>destination</th>\n",
       "      <th>net_sales</th>\n",
       "      <th>commision</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CBH</td>\n",
       "      <td>Travel Agency</td>\n",
       "      <td>Offline</td>\n",
       "      <td>Comprehensive Plan</td>\n",
       "      <td>No</td>\n",
       "      <td>186</td>\n",
       "      <td>MALAYSIA</td>\n",
       "      <td>-29.0</td>\n",
       "      <td>9.57</td>\n",
       "      <td>F</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CBH</td>\n",
       "      <td>Travel Agency</td>\n",
       "      <td>Offline</td>\n",
       "      <td>Comprehensive Plan</td>\n",
       "      <td>No</td>\n",
       "      <td>186</td>\n",
       "      <td>MALAYSIA</td>\n",
       "      <td>-29.0</td>\n",
       "      <td>9.57</td>\n",
       "      <td>F</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CWT</td>\n",
       "      <td>Travel Agency</td>\n",
       "      <td>Online</td>\n",
       "      <td>Rental Vehicle Excess Insurance</td>\n",
       "      <td>No</td>\n",
       "      <td>65</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>-49.5</td>\n",
       "      <td>29.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CWT</td>\n",
       "      <td>Travel Agency</td>\n",
       "      <td>Online</td>\n",
       "      <td>Rental Vehicle Excess Insurance</td>\n",
       "      <td>No</td>\n",
       "      <td>60</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>-39.6</td>\n",
       "      <td>23.76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CWT</td>\n",
       "      <td>Travel Agency</td>\n",
       "      <td>Online</td>\n",
       "      <td>Rental Vehicle Excess Insurance</td>\n",
       "      <td>No</td>\n",
       "      <td>79</td>\n",
       "      <td>ITALY</td>\n",
       "      <td>-19.8</td>\n",
       "      <td>11.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  agency    agency_type distribution                     product_name claim  \\\n",
       "0    CBH  Travel Agency      Offline               Comprehensive Plan    No   \n",
       "1    CBH  Travel Agency      Offline               Comprehensive Plan    No   \n",
       "2    CWT  Travel Agency       Online  Rental Vehicle Excess Insurance    No   \n",
       "3    CWT  Travel Agency       Online  Rental Vehicle Excess Insurance    No   \n",
       "4    CWT  Travel Agency       Online  Rental Vehicle Excess Insurance    No   \n",
       "\n",
       "   duration destination  net_sales  commision gender  age  \n",
       "0       186    MALAYSIA      -29.0       9.57      F   81  \n",
       "1       186    MALAYSIA      -29.0       9.57      F   71  \n",
       "2        65   AUSTRALIA      -49.5      29.70    NaN   32  \n",
       "3        60   AUSTRALIA      -39.6      23.76    NaN   32  \n",
       "4        79       ITALY      -19.8      11.88    NaN   41  "
      ]
     },
     "execution_count": 988,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(\n",
    "    columns={\n",
    "        'Agency': 'agency', \n",
    "        'Agency Type': 'agency_type', \n",
    "        'Distribution Channel': 'distribution', \n",
    "        'Product Name': 'product_name',\n",
    "        'Claim': 'claim',\n",
    "        'Duration': 'duration',\n",
    "        'Destination': 'destination',\n",
    "        'Net Sales': 'net_sales',\n",
    "        'Commision (in value)': 'commision',\n",
    "        'Gender': 'gender',\n",
    "        'Age': 'age'}, \n",
    "    inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1cc5c5",
   "metadata": {},
   "source": [
    "### Encoding Category Features\n",
    "Based on the unique values of each categorical column, we'll apply the following encoding for each column:\n",
    "- `agency`: we'll apply frequency encoding since all values have unique frequency digits\n",
    "- `agency_type`: representing 0 for airlines, and 1 for travel agency\n",
    "- `distribution_channel`: representing 0 for offline, and 1 for online\n",
    "- `product_name`: binary encoding; a better option than one-hot encoding because too many columns will be created based on the number of values.\n",
    "- `claim`: 0 for no, 1 for yes\n",
    "- `destination`: binary encoding; a better option than one-hot encoding because too many columns will be created based on the number of values.\n",
    "- `gender`: creating two columns (one-hot encoding), one for `male` and one for `female`. this would also address missing values at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 989,
   "id": "fbefcfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CBH' 'CWT' 'JZI' 'KML' 'EPX' 'C2B' 'JWT' 'RAB' 'SSI' 'ART' 'CSR' 'CCR'\n",
      " 'ADM' 'LWC' 'TTW' 'TST']\n",
      "EPX    35119\n",
      "CWT     8580\n",
      "C2B     8267\n",
      "JZI     6329\n",
      "SSI     1056\n",
      "JWT      749\n",
      "RAB      725\n",
      "LWC      689\n",
      "TST      528\n",
      "KML      392\n",
      "ART      331\n",
      "CCR      194\n",
      "CBH      101\n",
      "TTW       98\n",
      "CSR       86\n",
      "ADM       82\n",
      "Name: agency, dtype: int64\n",
      "['Travel Agency' 'Airlines']\n",
      "['Offline' 'Online']\n",
      "['Comprehensive Plan' 'Rental Vehicle Excess Insurance' 'Value Plan'\n",
      " 'Basic Plan' 'Premier Plan' '2 way Comprehensive Plan' 'Bronze Plan'\n",
      " 'Silver Plan' 'Annual Silver Plan' 'Cancellation Plan'\n",
      " '1 way Comprehensive Plan' 'Ticket Protector' '24 Protect' 'Gold Plan'\n",
      " 'Annual Gold Plan' 'Single Trip Travel Protect Silver'\n",
      " 'Individual Comprehensive Plan' 'Spouse or Parents Comprehensive Plan'\n",
      " 'Annual Travel Protect Silver' 'Single Trip Travel Protect Platinum'\n",
      " 'Annual Travel Protect Gold' 'Single Trip Travel Protect Gold'\n",
      " 'Annual Travel Protect Platinum' 'Child Comprehensive Plan'\n",
      " 'Travel Cruise Protect' 'Travel Cruise Protect Family']\n",
      "['No' 'Yes']\n",
      "['MALAYSIA' 'AUSTRALIA' 'ITALY' 'UNITED STATES' 'THAILAND'\n",
      " \"KOREA, DEMOCRATIC PEOPLE'S REPUBLIC OF\" 'NORWAY' 'VIET NAM' 'DENMARK'\n",
      " 'SINGAPORE' 'JAPAN' 'UNITED KINGDOM' 'INDONESIA' 'INDIA' 'CHINA' 'FRANCE'\n",
      " 'TAIWAN, PROVINCE OF CHINA' 'PHILIPPINES' 'MYANMAR' 'HONG KONG'\n",
      " 'KOREA, REPUBLIC OF' 'UNITED ARAB EMIRATES' 'NAMIBIA' 'NEW ZEALAND'\n",
      " 'COSTA RICA' 'BRUNEI DARUSSALAM' 'POLAND' 'SPAIN' 'CZECH REPUBLIC'\n",
      " 'GERMANY' 'SRI LANKA' 'CAMBODIA' 'AUSTRIA' 'SOUTH AFRICA'\n",
      " 'TANZANIA, UNITED REPUBLIC OF' \"LAO PEOPLE'S DEMOCRATIC REPUBLIC\" 'NEPAL'\n",
      " 'NETHERLANDS' 'MACAO' 'CROATIA' 'FINLAND' 'CANADA' 'TUNISIA'\n",
      " 'RUSSIAN FEDERATION' 'GREECE' 'BELGIUM' 'IRELAND' 'SWITZERLAND' 'CHILE'\n",
      " 'ISRAEL' 'BANGLADESH' 'ICELAND' 'PORTUGAL' 'ROMANIA' 'KENYA' 'GEORGIA'\n",
      " 'TURKEY' 'SWEDEN' 'MALDIVES' 'ESTONIA' 'SAUDI ARABIA' 'PAKISTAN' 'QATAR'\n",
      " 'PERU' 'LUXEMBOURG' 'MONGOLIA' 'ARGENTINA' 'CYPRUS' 'FIJI' 'BARBADOS'\n",
      " 'TRINIDAD AND TOBAGO' 'ETHIOPIA' 'PAPUA NEW GUINEA' 'SERBIA' 'JORDAN'\n",
      " 'ECUADOR' 'BENIN' 'OMAN' 'BAHRAIN' 'UGANDA' 'BRAZIL' 'MEXICO' 'HUNGARY'\n",
      " 'AZERBAIJAN' 'MOROCCO' 'URUGUAY' 'MAURITIUS' 'JAMAICA' 'KAZAKHSTAN'\n",
      " 'GHANA' 'UZBEKISTAN' 'SLOVENIA' 'KUWAIT' 'GUAM' 'BULGARIA' 'LITHUANIA'\n",
      " 'NEW CALEDONIA' 'EGYPT' 'ARMENIA' 'BOLIVIA' 'VIRGIN ISLANDS, U.S.'\n",
      " 'PANAMA' 'SIERRA LEONE' 'COLOMBIA' 'PUERTO RICO' 'UKRAINE' 'GUINEA'\n",
      " 'GUADELOUPE' 'MOLDOVA, REPUBLIC OF' 'GUYANA' 'LATVIA' 'ZIMBABWE'\n",
      " 'VANUATU' 'VENEZUELA' 'BOTSWANA' 'BERMUDA' 'MALI' 'KYRGYZSTAN'\n",
      " 'CAYMAN ISLANDS' 'MALTA' 'LEBANON' 'REUNION' 'SEYCHELLES' 'ZAMBIA'\n",
      " 'SAMOA' 'NORTHERN MARIANA ISLANDS' 'NIGERIA' 'DOMINICAN REPUBLIC'\n",
      " 'TAJIKISTAN' 'ALBANIA' 'MACEDONIA, THE FORMER YUGOSLAV REPUBLIC OF'\n",
      " 'LIBYAN ARAB JAMAHIRIYA' 'ANGOLA' 'BELARUS' 'TURKS AND CAICOS ISLANDS'\n",
      " 'FAROE ISLANDS' 'TURKMENISTAN' 'GUINEA-BISSAU' 'CAMEROON' 'BHUTAN'\n",
      " 'RWANDA' 'SOLOMON ISLANDS' 'IRAN, ISLAMIC REPUBLIC OF' 'GUATEMALA'\n",
      " 'FRENCH POLYNESIA' 'TIBET' 'SENEGAL' 'REPUBLIC OF MONTENEGRO'\n",
      " 'BOSNIA AND HERZEGOVINA']\n",
      "['F' nan 'M']\n"
     ]
    }
   ],
   "source": [
    "print(df['agency'].unique()) \n",
    "print(df['agency'].value_counts())\n",
    "print(df['agency_type'].unique())\n",
    "print(df['distribution'].unique())\n",
    "print(df['product_name'].unique())\n",
    "print(df['claim'].unique())\n",
    "print(df['destination'].unique())\n",
    "print(df['gender'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c84825",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features (Continued)\n",
    "After applying encoding to our categorical features, we can now see that all of our columns have numerical values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "id": "7cfbf77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 63326 entries, 0 to 63325\n",
      "Data columns (total 23 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   agency          63326 non-null  int64  \n",
      " 1   agency_type     63326 non-null  int64  \n",
      " 2   distribution    63326 non-null  int64  \n",
      " 3   product_name_0  63326 non-null  int64  \n",
      " 4   product_name_1  63326 non-null  int64  \n",
      " 5   product_name_2  63326 non-null  int64  \n",
      " 6   product_name_3  63326 non-null  int64  \n",
      " 7   product_name_4  63326 non-null  int64  \n",
      " 8   claim           63326 non-null  int64  \n",
      " 9   duration        63326 non-null  int64  \n",
      " 10  destination_0   63326 non-null  int64  \n",
      " 11  destination_1   63326 non-null  int64  \n",
      " 12  destination_2   63326 non-null  int64  \n",
      " 13  destination_3   63326 non-null  int64  \n",
      " 14  destination_4   63326 non-null  int64  \n",
      " 15  destination_5   63326 non-null  int64  \n",
      " 16  destination_6   63326 non-null  int64  \n",
      " 17  destination_7   63326 non-null  int64  \n",
      " 18  net_sales       63326 non-null  float64\n",
      " 19  commision       63326 non-null  float64\n",
      " 20  age             63326 non-null  int64  \n",
      " 21  male            63326 non-null  int64  \n",
      " 22  female          63326 non-null  int64  \n",
      "dtypes: float64(2), int64(21)\n",
      "memory usage: 11.1 MB\n"
     ]
    }
   ],
   "source": [
    "# installation instructions for category_encoders can be found here: https://github.com/scikit-learn-contrib/category_encoders\n",
    "from category_encoders import BinaryEncoder\n",
    "\n",
    "frequencies = df.groupby('agency').size()\n",
    "df['agency'] = df['agency'].map(frequencies)\n",
    "\n",
    "df['agency_type'] = (df['agency_type'] == 'Travel Agency').astype(int)\n",
    "df['distribution'] = (df['distribution'] == 'Online').astype(int)\n",
    "df['claim'] = (df['claim'] == 'Yes').astype(int)\n",
    "df['male'] = (df['gender'] == 'M').astype(int)\n",
    "df['female'] = (df['gender'] == 'F').astype(int)\n",
    "df.drop(columns='gender', inplace=True)\n",
    "\n",
    "\n",
    "encoder = BinaryEncoder(cols=['product_name', 'destination'])\n",
    "data_encoded = encoder.fit_transform(df)\n",
    "df = data_encoded\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319192a8",
   "metadata": {},
   "source": [
    "### Resolving Positive Skewness\n",
    "As mentioned during our data understanding earlier, the column values `Duration`, `Net Sales`, `Commision (in value)`, and `Age` are highly positively skewed. So we'll need to resolve that by applying Winorization before moving onto our prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "id": "c7b66e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49614889854199307\n",
      "0.44424846894896575\n",
      "0.48540294373837195\n",
      "0.44853294623002843\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "temp_df = df.copy()\n",
    "temp_df['duration'] = winsorize(temp_df['duration'], (0.1, 0.2))\n",
    "temp_df['net_sales'] = winsorize(temp_df['net_sales'], (0.1, 0.2))\n",
    "temp_df['commision'] = winsorize(temp_df['commision'], (0.1, 0.26))\n",
    "temp_df['age'] = winsorize(temp_df['age'], (0.1, 0.153))\n",
    "\n",
    "print(temp_df['duration'].skew()) # skew value of 0.496\n",
    "print(temp_df['net_sales'].skew()) # skew value of 0.444\n",
    "print(temp_df['commision'].skew()) # skew value of 0.485\n",
    "print(temp_df['age'].skew()) # skew value of 0.449\n",
    "\n",
    "df['duration'] = winsorize(df['duration'], (0.1, 0.2))\n",
    "df['net_sales'] = winsorize(df['net_sales'], (0.1, 0.2))\n",
    "df['commision'] = winsorize(df['commision'], (0.1, 0.26))\n",
    "df['age'] = winsorize(df['age'], (0.1, 0.153))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c47d9bc",
   "metadata": {},
   "source": [
    "## Prediction Model\n",
    "Alas, we're done with the feature engineering portion. We can now move on to creating a prediction model for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96348ec6",
   "metadata": {},
   "source": [
    "### Random Forest Classifier - Setting up function\n",
    "We'll get started with setting up the Random Forest classifier model. To prevent redundant effort in this notebook with predicting training data and/or testing data, a function is created to handle all the logic. An analysis of the results will be covered shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "id": "5eb14e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.metrics as metric\n",
    "from IPython.display import display\n",
    "\n",
    "numerical_features = ['duration', 'net_sales', 'commision', 'age']\n",
    "categorical_features = [\n",
    "    'agency', \n",
    "    'agency_type', \n",
    "    'distribution', \n",
    "    'product_name_0', \n",
    "    'product_name_1', \n",
    "    'product_name_2', \n",
    "    'product_name_3', \n",
    "    'product_name_4', \n",
    "    'destination_0', \n",
    "    'destination_1', \n",
    "    'destination_2', \n",
    "    'destination_3', \n",
    "    'destination_4', \n",
    "    'destination_5', \n",
    "    'destination_6', \n",
    "    'destination_7', \n",
    "    'male', \n",
    "    'female'\n",
    "] \n",
    "\n",
    "X = df[numerical_features + categorical_features]\n",
    "Y = df['claim']\n",
    "\n",
    "def rfPredictAndShowScores(test_size: float = 0.10, use_test_data: bool = False):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=100)\n",
    "    rf = RandomForestClassifier()\n",
    "    y_predicted = []\n",
    "    if use_test_data:\n",
    "        rf.fit(x_test, y_test)\n",
    "        y_predicted = rf.predict(x_test)\n",
    "        accuracy_rf = np.round(metric.accuracy_score(y_true=y_test, y_pred=y_predicted), decimals=3)\n",
    "        precision_rf = np.round(metric.precision_score(y_true=y_test, y_pred=y_predicted), decimals=3)\n",
    "        recall_rf = np.round(metric.recall_score(y_true=y_test, y_pred=y_predicted), decimals=3)\n",
    "        display(pd.Series(data=rf.feature_importances_, index=x_test.columns).sort_values(ascending=False).round(3))\n",
    "    else:\n",
    "        rf.fit(x_train, y_train)\n",
    "        y_predicted = rf.predict(x_train)\n",
    "        accuracy_rf = np.round(metric.accuracy_score(y_true=y_train, y_pred=y_predicted), decimals=3)\n",
    "        precision_rf = np.round(metric.precision_score(y_true=y_train, y_pred=y_predicted), decimals=3)\n",
    "        recall_rf = np.round(metric.recall_score(y_true=y_train, y_pred=y_predicted), decimals=3)\n",
    "        display(pd.Series(data=rf.feature_importances_, index=x_train.columns).sort_values(ascending=False).round(3))\n",
    "    print('Accuracy:', accuracy_rf)\n",
    "    print('Precision:', precision_rf)\n",
    "    print('Recall', recall_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "id": "73098e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificationMetrics(y_test, y_pred):\n",
    "    accuracy = np.round(metric.accuracy_score(y_true=y_test, y_pred=y_pred), decimals=3)\n",
    "    precision = np.round(metric.precision_score(y_true=y_test, y_pred=y_pred), decimals=3)\n",
    "    recall = np.round(metric.recall_score(y_true=y_test, y_pred=y_pred), decimals=3)\n",
    "\n",
    "    return { 'accuracy': accuracy, 'precision': precision, 'recall': recall }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b483a83c",
   "metadata": {},
   "source": [
    "### Results Evaluation\n",
    "Our random forest classifier model has high `accuracy`, but that metric isn't enough to determine our model's performance given our prediction is a classification problem.\n",
    "\n",
    "Hence, we'll make sure to include both `precision` and `recall` metrics. \n",
    "\n",
    "**Precision** in this case will quanity the *correct positive predictions made* whereas **recall** will quantify the *number of correct positive predictions made out of all positive predictions that could have been made (taking into account true positive and false negatives)* (Brownlee, 2020).\n",
    "\n",
    "Looking at the results, we can see our precision is high but our recall isn't as high. Our recall is higher with our testing data too versus our training data. Most likely, what may be contributing to a low recall value is the fact that we have imbalanced classification within our data since most values of `claim` are `No` instead of `Yes`.\n",
    "\n",
    "Another observation is the feature importance for both training and test data. We can see that features that are most relevant for predicting `claim` are `duration` and `age`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "id": "838f81a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data stats\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "duration          0.403\n",
       "age               0.244\n",
       "net_sales         0.159\n",
       "commision         0.050\n",
       "agency            0.016\n",
       "destination_5     0.016\n",
       "destination_4     0.015\n",
       "destination_7     0.013\n",
       "destination_3     0.012\n",
       "destination_6     0.011\n",
       "female            0.008\n",
       "product_name_4    0.008\n",
       "product_name_2    0.007\n",
       "male              0.007\n",
       "product_name_3    0.007\n",
       "product_name_1    0.007\n",
       "destination_2     0.007\n",
       "agency_type       0.005\n",
       "distribution      0.004\n",
       "product_name_0    0.002\n",
       "destination_1     0.001\n",
       "destination_0     0.000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.993\n",
      "Precision: 0.967\n",
      "Recall 0.529\n",
      "\n",
      "Testing data stats\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "duration          0.300\n",
       "age               0.239\n",
       "net_sales         0.159\n",
       "commision         0.087\n",
       "product_name_1    0.020\n",
       "product_name_2    0.019\n",
       "agency            0.018\n",
       "female            0.016\n",
       "male              0.016\n",
       "product_name_4    0.015\n",
       "destination_7     0.015\n",
       "destination_6     0.015\n",
       "destination_4     0.015\n",
       "destination_5     0.013\n",
       "product_name_3    0.012\n",
       "destination_3     0.012\n",
       "destination_2     0.011\n",
       "agency_type       0.007\n",
       "destination_1     0.005\n",
       "product_name_0    0.004\n",
       "distribution      0.001\n",
       "destination_0     0.000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.997\n",
      "Precision: 1.0\n",
      "Recall 0.819\n"
     ]
    }
   ],
   "source": [
    "print('Training data stats')\n",
    "rfPredictAndShowScores()\n",
    "\n",
    "print('\\nTesting data stats')\n",
    "rfPredictAndShowScores(use_test_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b9074c",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "To get an even more vivid picture of why accuracy alone shouldn't be relied upon, we've setup a confusion matrix. Here's the breakdown of the results based on our test data:\n",
    "- `True Negatives`: 12,442 - the actual value is Claim Unfiled, and the predicted value is Claim Unfiled\n",
    "- `False Positives`: 33 - the actual value is Claim Unfiled, and the predicted value is Claim Filed\n",
    "- `False Negatives`: 187 - the actual value is Claim Filed, and the predicted value is Claim Unfiled\n",
    "- `True Positives`: 4 - the actual value is Claim Filed, and the predicted value is Claim Filed\n",
    "\n",
    "We can seee based on these results that imbalanced classification truly plays a role. Compared to total `Claims Not Filed` (12,475), `Claims Filed` (191) is truly imbalanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "id": "6ed38f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Pred Claim Unfiled  Pred Claim Filed\n",
      "Obs Claim Unfiled               12444                31\n",
      "Obs Claim Filed                   187                 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def displayConfusionMatrix(x_test, y_test, model):\n",
    "    threshold = 0.5\n",
    "    y_pred_prob = model.predict_proba(x_test)[:, 1]\n",
    "    y_pred = (y_pred_prob > threshold).astype(int)\n",
    "    matrix = confusion_matrix(y_test, y_pred)\n",
    "    matrix_df = pd.DataFrame(matrix, index=[\"Obs Claim Unfiled\", \"Obs Claim Filed\"], columns=[\"Pred Claim Unfiled\", \"Pred Claim Filed\"])\n",
    "    print(matrix_df)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=100)\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(x_train, y_train)\n",
    "displayConfusionMatrix(x_test, y_test, rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc26246d",
   "metadata": {},
   "source": [
    "### Undersampling Data\n",
    "In order to address the shortcomings of our model, we'll proceeed with applying undersampling.\n",
    "\n",
    "We can see after applying undersampling, `Claim Filed` is at 927, while `Claim Not Filed` is down to 1158 - 20% more than `Claim Filed`. The reason for going with 80% as our sampling strategy (20% more for `Claim Not Failed`) is just so we can ensure accuracy doesn't end up being much less than our prior model setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "id": "49bc3c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Y: Counter({0: 62399, 1: 927})\n",
      "Y after undersampling: Counter({0: 1158, 1: 927})\n"
     ]
    }
   ],
   "source": [
    "# Install instructions here: https://imbalanced-learn.org/stable/install.html#getting-started\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "# RandomUnderSampler(sampling_strategy=0.80)\n",
    "rus = RandomUnderSampler(sampling_strategy=0.80)\n",
    "x_rus, y_rus = rus.fit_resample(X, Y)\n",
    "\n",
    "print('Original Y:', Counter(Y))\n",
    "print('Y after undersampling:', Counter(y_rus))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_rus, y_rus, test_size=0.20, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2785a606",
   "metadata": {},
   "source": [
    "### Confusion Matrix (Post Undersampling)\n",
    "Taking a look at both classification metrics and the confusion matrix after undersampling, we can clearly see different results.\n",
    "\n",
    "For accuracy, precision, and recall, they have all decreased. But that should be expected and nothing we should be too concerned about for now given that our previous model prediction encountered imbalanced classification.\n",
    "\n",
    "Looking at the confusing matrix, here's what we have:\n",
    "- `True Negatives`: 198 - the actual value is Claim Unfiled, and the predicted value is Claim Unfiled\n",
    "- `False Positives`: 38 - the actual value is Claim Unfiled, and the predicted value is Claim Filed\n",
    "- `False Negatives`: 70 - the actual value is Claim Filed, and the predicted value is Claim Unfiled\n",
    "- `True Positives`: 111 - the actual value is Claim Filed, and the predicted value is Claim Filed\n",
    "\n",
    "We can clearly see the effect of undersampling reduced the number of false negatives and increased the number of true positives! \n",
    "\n",
    "However, our work isn't done here yet. We'll need to try tuning our model using hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "id": "2ccafe09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "{'accuracy': 0.707, 'precision': 0.671, 'recall': 0.641}\n",
      "                   Pred Claim Unfiled  Pred Claim Filed\n",
      "Obs Claim Unfiled                 179                57\n",
      "Obs Claim Filed                    65               116\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest')\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "print(classificationMetrics(y_test, y_pred))\n",
    "displayConfusionMatrix(x_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708a7604",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "**NOTE**: Running this will take about 5-6 minutes more or less.\n",
    "\n",
    "In order to make the most of hyperparameter tuning with the `RandomForestClassifier` predictive model, we'll want to select the appropriate hyperparameters while applying a k fold using `RepeatedStratifiedKFold` (Brownlee, 2020), then utilizing grid search to determine the best score for `precision` and `recall`.\n",
    "\n",
    "After getting the best scores, we notice both `precision` and `recall` scores perform best with the following hyperparameters: `class_weight: 'balanced', n_estimators: 500`. However it seems that for `max_features` `recall` performs best with `sqrt` whereas `precision` performs best with `log2`.\n",
    "\n",
    "Ideally, it would be best to strive between a great `recall` and great `precision` scores. We'll cover that next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best recall: 0.685471 using {'class_weight': 'balanced', 'max_features': 'log2', 'n_estimators': 1000}\n",
      "Best precision: 0.699760 using {'class_weight': 'balanced', 'max_features': 'sqrt', 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "n_estimators = [100, 500, 1000]\n",
    "class_weight = ['balanced', None]\n",
    "max_features = ['sqrt', 'log2']\n",
    "\n",
    "grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'class_weight': class_weight,\n",
    "    'max_features': max_features\n",
    "}\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='recall',error_score=0)\n",
    "grid_result = grid_search.fit(x_train, y_train)\n",
    "print(\"Best recall: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='precision',error_score=0)\n",
    "grid_result = grid_search.fit(x_train, y_train)\n",
    "print(\"Best precision: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64706a9",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning (Continued)\n",
    "With give hyperparamter tuning another shot, but this time with `f1` score. F1 would be an ideal classification metric to strive for because it's a mean of both recall and precision. Since we'd prefer an ideal balance of both precision and recall, F1 score would be the way to go.\n",
    "\n",
    "We can see that we get the best F1 score with the following hyperparameters:\n",
    "- `class_weight: None`\n",
    "- `max_features: sqrt`\n",
    "- `n_estimators: 1000`\n",
    "\n",
    "Now that we have the best hyperparamters to utilize for our prediction model, we'll wrap up soon with a final prediction and confusion matrix evalution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 999,
   "id": "2bf0adf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best recall: 0.691261 using {'class_weight': 'balanced', 'max_features': 'log2', 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='f1',error_score=0)\n",
    "grid_result = grid_search.fit(x_train, y_train)\n",
    "print(\"Best recall: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8750ddc",
   "metadata": {},
   "source": [
    "### Prediction and Confusion Matrix Wrap Up\n",
    "While not perfect nor much improvement compared to our previous confusion matrix, it was worth going through the effort of hyperparameter tuning.\n",
    "\n",
    "From the confusion matrix, we can see our true negatives went from 177 to 176, while our true positives went from 116 to 117."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "id": "25f7d6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Pred Claim Unfiled  Pred Claim Filed\n",
      "Obs Claim Unfiled                 176                60\n",
      "Obs Claim Filed                    64               117\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=1000, max_features='sqrt')\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "displayConfusionMatrix(x_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8be674",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Bringing it altogether, we've done the following to successfully create a classification model:\n",
    "- Understand the data\n",
    "- Apply feature engineering to all categorical columns\n",
    "- Address skewness for continuous columns\n",
    "- Utilized Random Forester to create a classification model for our target `claim`\n",
    "- Analyze and notate observations of model performance via metrics such as accuracy, precision, and recall\n",
    "- Utilize a confusion matrix on why accuracy alone isn't enough\n",
    "- Utilize hyperparamter tuning and cross validation to get the best F1 score - a balance of both precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e3375a",
   "metadata": {},
   "source": [
    "# References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b3adf8",
   "metadata": {},
   "source": [
    "Brownlee, J. (2019, June 20). *Classification Accuracy is Not Enough: More Performance Measures You*\\\n",
    "&emsp; *Can Use*. Machine Learning Mastery. Retrieved March 10, 2022, from\\\n",
    "&emsp; https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/ \n",
    "\n",
    "Brownlee, J. (2020, January 15). *Random Oversampling and Undersampling for Imbalanced Classification*\\\n",
    "&emsp; Machine Learning Mastery. Retrieved March 18, 2022, from\\\n",
    "&emsp; https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c5754c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
